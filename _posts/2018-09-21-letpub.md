---
layout: post
title: "一个爬虫"
date: 2018.09.21 14:00
categories: script
tag: python
---
* content
{:toc}


洁哥需要国家自然科学基金立项列表，于是有了这个爬虫脚本。

.gov官网的还是不要动手比较好，这次从第三方网站爬取数据。

思路很简洁，指定url发送get请求，得到结果解析一下提取需要的内容写到文本里。所有要get的URL符合统一规则，可以直接循环，不需要跳链。可以说十分简单了。

不过，爬了一会发现网站限制ip访问次数。。。不得已用代理IP池搞了，这已经很暴力了，实际需要的数据量又没有那么多，所以单线程就行了没有写并发。

又过了一会发现免费的ip代理真的是成功率低到离谱，所以换了个收费稳定版的，洁哥红包报销。

自用版的涉及收费代理内容和公司内容不上源码了，github上传了一个调试时的DEMO版，还在调试的时候没有精简，看起来粗糙不堪，改好的又不能传，就记个意思吧。

DEMO版用了免费的ip代理，可以每分钟刷新代理ip<del>，虽然刷新也没啥用</del>，异常处理没写全，对于测试够用了。URL列表基于网站结构解析结果截取了一小段存到了固定的字典里，也可以用网站结构解析脚本替换成返回值形式的。IP访问超限自动删除，超时重复等待50次。还有个有效IP添加白名单反复调用的，好像那会还没写进去，另加个列表就行了。

最后爬取了小几万条结果<font color="#33FF00">，没爬完</font>，用户还是很满意的，毕竟收了10元红包，不满意就改到满意为止。爬下来的数据里居然有重复的结果。。。按日志查了下，确实是网站有问题，最后按照项目名或项目号去下重复就好了。后来发现这网站里的数据还有自带乱码的，太不走心了。

[这个是脚本](https://github.com/captorr/ngs/blob/master/scripts/letpub.py)